[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ZVX108E Hydrology",
    "section": "",
    "text": "Introduction\nThe ZVX108E Hydrology taught at the Czech University of Life Sciences is an introductory undergraduate course with students of various scientific background, many of whom do not possess knowledge of any programming language yet. Therefore students are introduced within the sessions to the R programming language, which serves for decades as a great tool for scientific data processing, statistical evaluation, visualization and reporting. The choice of R over other tools (like Matlab, Python or Julia) is opinionated. R is told to be “developed by statisticians for statisticians” and as such fits well into the workflow of hydrological data processing, which in most of scenarios start with some statistical analysis. The language is more straightforward to learn than others, especially for non-programmers and does not maim a student’s workflow with plethora of environments and mutually incompatible package versions.\nSince the course only contains six practical sessions, oriented to various parts of hydrology, we focus on the basic and merit. The text starts with a fairly fast-paced introduction to R language and it is due to this, that it did not end far from it. We purposefully avoid using what is considered a modern approach in data workflow (mainly with the help from tidyverse & data.table) as well using native pipe |&gt; operator, a syntactical feature from (R&gt;=4.1.0) which we consider too big of a change of the programming paradigm for a newcomer\nThis text was created with the use of 4.4.2 and namespaces of following packages:\n\n\n             Version\nbase           4.4.2\ncli            3.6.3\ncompiler       4.4.2\ndatasets       4.4.2\ndigest        0.6.37\nevaluate       1.0.1\nfastmap        1.2.0\nglue           1.8.0\ngraphics       4.4.2\ngrDevices      4.4.2\nhtmltools    0.5.8.1\nhtmlwidgets    1.6.4\njsonlite       1.8.9\nknitr           1.49\nlifecycle      1.0.4\nmagrittr       2.0.3\nmethods        4.4.2\nrlang          1.1.4\nrmarkdown       2.29\nrstudioapi    0.17.1\nstats          4.4.2\nstringi        1.8.4\nstringr        1.5.1\ntools          4.4.2\nutils          4.4.2\nwebexercises   1.1.0\nxfun            0.49\n\n\nReproduction of all the materials should be possible using the same versions."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Data",
    "section": "",
    "text": "A list of datasets, used within the sessions.\n\nPřehled datových sad ke cvičením.\n\n\nChapter\nFile\nDescription\nReference\n\n\n\n\nChapter 3 Excercise 2\n\nDaily time series from MOPEX dataset in *.fwf format\n-\n\n\nChapter 3 Exercise 3\n\nDaily time series with metadata. Czech Hydrometeorological Institute\n\n\n\n?sec-hydroklimatickeindexy\n\nPrůměrný denní průtok 11/1991 - 12/2023 QD165600 - Louňovice pod Blaníkem\nČeský hydrometeorologický ústav\nPodmínky užití"
  },
  {
    "objectID": "01_R.html#getting-help",
    "href": "01_R.html#getting-help",
    "title": "2  Introduction to R language",
    "section": "2.1 Getting help",
    "text": "2.1 Getting help\nIt is entered into the console in the form help(&lt;function name&gt;) or ?&lt;function name&gt;. If we would like to look directly into the code of the function, it is also possible, we just enter the name of the function in the console without brackets, or use the command View(&lt;function name&gt;). In addition, R also has help.search(&lt;function name&gt;) under the shortcut ??, which searches for full-text help across installed packages. Furthermore, it is still possible to search the R language mailing list using the function RSiteSearch(), which opens a new window of the predefined browser. In addition, thematically integrated help cards are very useful: ?Logical, ?Constants, ?Control, ?Arithmetic, ?Syntax, ?Special etc.\n\n\n\n\n\n\nExercise\n\n\n\nTry to find help to DateTimeClasses. a) What do the POSIXct a POSIXlt represent? b) What is the difference between them? c) Find a function for calculating \\(5!\\)"
  },
  {
    "objectID": "01_R.html#r-as-scientific-calculator",
    "href": "01_R.html#r-as-scientific-calculator",
    "title": "2  Introduction to R language",
    "section": "2.2 R as scientific calculator",
    "text": "2.2 R as scientific calculator\n\n2.2.1 Arithmetic operations\n\n\nCode\n1 + 2           # addition\n## [1] 3\n1 - 2           # subtraction \n## [1] -1\n1 / 2           # division\n## [1] 0.5\n1 * 2           # multiplication\n## [1] 2\n1 %/% 2         # integer division\n## [1] 0\n1 %% 2          # modulo oprator\n## [1] 1\n\n\n\n\n2.2.2 Special values\nR is familiar with the concept of \\(\\pm\\infty\\), hence -Inf and Inf values are at disposal. You will get them most probably as results from computation heading to \\(\\frac{\\pm1}{0}\\) numerically. There are other special values like NULL (null value), NA (not assigned) and NaN (not a number). The concept of not assigned is one that is particularly important, since it has significant impact on the computed result ({(code-mean-rm?)}). NA is of default type logical. Otherwise it si possible to specify missing value in all other data type like NA_real_ (matches double), NA_integer_, NA_complex_ and NA_character_, these are all usable in pre-allocation of memory for data structures. Try to find the usage of functions na.omit(), is.na(), complete.cases().\n\n\nCode\n1x &lt;- seq(1:10)\n2x[c(5,6)] &lt;- NA\nprint(x)\n3mean(x)\n4mean(x, na.rm = TRUE)\n\n\n\n1\n\nGeneral sequence of numbers\n\n2\n\nchange some elements to not assigned\n\n3\n\nwithout removal\n\n4\n\nand with removal\n\n\n\n\n [1]  1  2  3  4 NA NA  7  8  9 10\n[1] NA\n[1] 5.5\n\n\n\n\n2.2.3 Set operations\nFor manipulating sets, there are a couple of essential functions union(), intersect(), setdiff() and operator %in%.\n\n\nCode\nset_A &lt;- c(\"a\", \"a\", \"b\", \"c\", \"D\")\nset_B &lt;- c(\"a\", \"b\", \"d\")\nunion(set_A, set_B)\n## [1] \"a\" \"b\" \"c\" \"D\" \"d\"\nintersect(set_A, set_B)\n## [1] \"a\" \"b\"\nset_A %in% set_B\n## [1]  TRUE  TRUE  TRUE FALSE FALSE\n\n\nThe operators fall in arithmetic, relation, assign categories and we also put set functions here.\n\n\n\n\n\n\n\nSign\nMeaning\n\n\n\n\n+ , - , * , / , %% , %/% , ** nebo ^, %*%\narithmetic operators (plus, minus, multiply, divide, modulo, integer division, power and matrix multiplication)\n\n\n&gt; ,&gt;= , &lt; , &lt;= , == , !=\nrelation operators (larger/smaller than, equal, not equal)\n\n\n! , & , && , | , ||\nlogical (negation, and, or)\n\n\n~\nfunctional relationship\n\n\n&lt;- , =, &lt;&lt;-, -&gt;\nassign operator\n\n\n$\nnaming indexation in heterogenic structures\n\n\n:\nrangea\n\n\nisTRUE() , all() , any() , %in% , setdiff()\nset functions\n\n\n\n\n\n2.2.4 Mathematical functions\n\n\n\n\n\n\n\nFunction\nMeaning\n\n\n\n\nlog(x)\nlogarithm \\(x\\) to the base \\(e\\)\n\n\nexp(x)\n\\(x(e^x)\\)\n\n\nlog(x, n)\nlogarithm \\(x\\) base \\(n\\)\n\n\nlog10(x)\nlogarithm \\(x\\) base \\(10\\)\n\n\nsqrt(x)\nsquare root from \\(x\\)\n\n\nfactorial(x)\n\\(x!\\)\n\n\nchoose(n, x)\nbinomial coefficients\n\\[\n                                                                                                                                                                                                                                                                                                            \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n                                                                                                                                                                                                                                                                                                         \\]\n\n\nceiling(x)\nsmallest integer large than \\(x\\)\n\n\nfloor(x)\nlargest integer before \\(x\\)\n\n\ntrunc(x)\nclosest number between \\(x\\) a 0\n\n\nround(x, digits)\nround \\(x\\) to \\(n\\) decimals\n\n\nsignif(x, digits)\nround \\(x\\) to \\(n\\) significant numbers\n\n\ncos(x) , sin(x) , tan(x)\nfunction ins rad\n\n\nacos(x) , asin(x) , atan(x)\ninverse trigonometric functions\n\n\nabs(x)\nabsolute value\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nEvaluate the following expressions:\na) \\(1 + 3 \\cdot (2 / 3)\\:\\mathrm{mod}\\:3\\)\nb) \\(\\dfrac{\\sin(2.3)}{\\cos(\\pi)}\\)\nc) \\(\\sum\\limits_{i = 1}^{53}i\\)\nd) \\(\\dfrac{-\\infty}{0}\\), \\(\\dfrac{-\\infty}{\\infty}\\), \\(\\dfrac{0}{0}\\)\ne) \\(\\left(\\dfrac{2}{35}\\right)^{0.5} \\cdot 3 \\cdot (2 / 3)\\)\nf) \\(20!\\)\ng) \\(\\int_{0}^{3\\pi} \\sin(x) dx\\)\n\n\n\nMatrix operations\nLet’s say we have a set of linear equations\n\\[\n\\begin{matrix}\n2x& - 3y& &= 3\\\\\n& - 2y& + 4z &= 9\\\\\n2x& + 13y& + 9z&= 10\n\\end{matrix}\\\\\n\\tag{2.1}\\]\nSolving {Equation 2.1} is a one-liner:\n\n\nCode\nA &lt;- matrix(data = c(2, -3, 0, 0, -2, 4, 2, 13, 9), nrow = 3, byrow = TRUE)\nB &lt;- c(3, 9, 10)\nsolve(A, B)\n\n\n[1]  0.5304878 -0.6463415  1.9268293"
  },
  {
    "objectID": "01_R.html#r-as-programming-language",
    "href": "01_R.html#r-as-programming-language",
    "title": "2  Introduction to R language",
    "section": "2.3 R as programming language",
    "text": "2.3 R as programming language\n\n2.3.1 Variables and name conventions\nIt is highly discouraged using spaces and diacritical marks in naming, like the Czech translation of the term “variable” - proměnná. Most programmers use either camelNotation or snake_notation for naming purposes. Obviously the R is case-sensitive so camelNotation and CamelNotation are two different things. Variables do not contain spaces, quotes, arithmetical, logical nor relational operators neither they contain special characters like =, -, ``. Objects cannot be named by key words.\n\nKey words\nif, else, repeat, while, function, for, in, next, repeat, break, TRUE, FALSE, NULL, Inf, NaN, NA, NA_integer_, NA_real_, NA_complex_, NA_character_\nIt is not recommended to inlude dot in the name, like morava.prutoky, and to match the names with commonly used functions. R is “case-sensitive” which means, that X does not equal x.\n\n\nExercise\nIntuitively, we might be guided to load the data into the data variable. This is the wrong however, since data() is a function to access datasets that are part of the basic R installation. Try it out.\n\n\nSome cases of possible but wrong naming\naaa, Morávka průtok [m/s], moje.proměnná\n\n\n\n\n2.3.2 Rules of quotation marks and parenthesses\nBoth represent the paired characters in R. Parenthesses are used in three versions: classical, square brackets and curly brackets (braces). All of them have specific non-overlaping usage.\n\n() are always to be found right next to a function name they delineate the space where function arguments are to be specified.\n\n[] are always use with the name of the object (vector, array, list, …) and signalize subselecting from the object.\n\n{} mark a block of code, which should be executed at once.\n\n\nQuotation marks introduce text strings. Both “double” and ‘single’ quotes can be used completely at will, they just need to be closed with the same type. Back quotes are also common and are used, for example, to delimit a non-standard column name in a structure.\n\n\n2.3.3 Functions\nYou can define own functions using the function() construct. If you work in ****RStudio, just type fun and tabulate a snippet from the IDE help. The action produces {(code-function-snippet?)}.\n\n\nCode\nname &lt;- function(variables) {\n  ...\n}\n\n\nname is the name of the function we would like to create and variables are the arguments of that function. Space between the {and } is called a body of a function and contains all the computation which is invoked when the function is called.\nLet’s put Here an example of creating own function to calculate weighted mean\n\\[\n\\bar{x} = \\dfrac{\\sum\\limits_{i=1}^{n} w_ix_i}{\\sum\\limits_{i=1}^{n}w_i},\n\\] where \\(x_iw_i\\) are the individual weighted measurements.\nWe define a simple function for that purpose and run an example.\n\n\nCode\nw_mean &lt;- function(x, w = 1/length(x)) {\n  sum(x*w)/sum(w)\n}\nw_mean(1:10)\n\n\n[1] 55\n\n\nHere is a different example:\n\n\nCode\nx &lt;- rnorm(100)\n1nejblizsi_hodnota &lt;- function(x, value) {\n  x[which(abs(x - value) == min(abs(x - value)))]\n}\n\ncat(\"Hodnota nejblíže 0 z vektoru x je:\" , nejblizsi_hodnota(x = x, value = 0))\n\n\n\n1\n\nExample of function, which seeks the neares number from a vector x to a certain referential value.\n\n\n\n\nHodnota nejblíže 0 z vektoru x je: 0.01881842\n\n\nWe can test if we get the same result as the primitive function from R using all.equal() statement.\n\n\nCode\nall.equal(w_mean(x = 1:5, w = c(0.25, 0.25, 1, 2, 3)), \n          weighted.mean(x = 1:5, w = c(0.25, 0.25, 1, 2, 3)))\n\n\n[1] TRUE\n\n\nAny argument without default value in the function definition has to be provided on function call. You can frequently see functions with the possibility to specify ... a so-called three dot construct or ellipsis. The ellipsis allows for adding any number of arguments to a function call, after all the named ones.\n\n\n2.3.4 Data types\nThe basic types are logical, integer, numeric, complex, character and raw. There are some additional types which we will encounter like Date. Since R is dynamically typed, it is not necessary for the user to declare variables before using them. Also the type changes without notice based on the stored values, where the chain goes from the least complex to the most. The summary is in the following table\n\n\nCode\nTRUE    # logical, also T as short version\n## [1] TRUE\n1L      # integer\n## [1] 1\n1.2     # numeric\n## [1] 1.2\n1+3i    # complex\n## [1] 1+3i\n\"A\"     # character, also 'A'\n## [1] \"A\"\n\n\nThey represent the individual elements of data structures. R dynamically typed and does not require declarations before usage.\n\nBasic types and coercions.\n\n\n\n\n\n\n\n\n\n\n\nlogical\ninteger\nnumeric\ncomplex\ncharacter\n\n\n\n\nlogical\nlogical\ninteger\nnumeric\ncomplex\ncharacter\n\n\ninteger\nlogical\ninteger\nnumeric\ncomplex\ncharacter\n\n\nnumeric\nlogical\nnumeric\nnumeric\ncomplex\ncharacter\n\n\ncomplex\nlogical\ninteger + warning\nnumeric + warning\ncomplex\ncharacter\n\n\ncharacter\nNA_logical\nNA_integer + warning\nNA_numeric + warning\nNA_complex + warning\ncharacter\n\n\n\nTwo types of functions are connected to data types: is.___ a as.___. Is is either questioning or coertion of data type. Try also class(), mode().\n\n\nCode\nis.character(\"ABC\")\n\n\n[1] TRUE\n\n\nCode\nas.integer(11 + 1i)\n\n\nWarning: imaginary parts discarded in coercion\n\n\n[1] 11\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCreate in any way a vector x of 10 different numerical values, where \\(x\\in\\mathbb{R}\\).\nWrite an expression to select numbers between -5 and 5 from this vector.\nConvert to integer type and discuss the result.\nAdd 3 positions “A”, “B” and “C” to the vector, has the vector changed?\n\n\n\n\n\n2.3.5 Data structures\n\nVectors\nAtomic vectors are single-type linear structures. They can contain elements of any type, from logical, integer, numeric, complex, character. A vector is a basic building structure in the R language, there is nothing like a scalar quantity here. The concept of vector is understood here in the mathematical sense as a vector of values representing a point in \\(n\\)-dimensional space.\n\\[\n\\mathbf{\\mathrm{u}} =\n\\begin{pmatrix}\n1\\\\\n1.5\\\\\n-14\\\\\n7.223\\\\\n\\end{pmatrix}, \\qquad\n\\mathbf{\\mathrm{v}} =\n\\begin{pmatrix}\n\\mathrm{TRUE}\\\\\n\\mathrm{FALSE}\\\\\n\\mathrm{TRUE}\\\\\n\\mathrm{TRUE}\\\\\n\\end{pmatrix}, \\qquad\n\\mathbf{\\mathrm{u^T}} =\n\\begin{pmatrix}\n1 & 1.5 & -14 & 7.233\\\\\n\\end{pmatrix}\n\\]\nMany functions lead to creation of a vector, among the most used are vector(mode = \"numeric\", length = 10), function c(), or using subset operators [ or [[.\nAn important rule is tied to vectors - value recycling.\n\n\nCode\nv &lt;- c(1.4, 2.0, 6.1, 2.7)\nu &lt;- c(2.0, 1.3)\n1u + v\n2u * v\n3u * 2.3\n\n\n\n1\n\nAdding two vectors while length of second is the multiple of the first\n\n2\n\nMultiplying two vectors while length of second is the multiple of the first\n\n3\n\nMultipling with single numeric value\n\n\n\n\n[1] 3.4 3.3 8.1 4.0\n[1]  2.80  2.60 12.20  3.51\n[1] 4.60 2.99\n\n\n\nWorking with vectors\n\n\nCode\n1x &lt;- 1:10\nx &lt;- seq(10:1)\nx &lt;- vector(mode = \"numeric\", length = 10)\nx &lt;- replicate(n = 10, expr = eval(2))\nx &lt;- sample(x = 10, size = 10, replace = TRUE)\nx &lt;- rep(x = 15, times = 2)\nx &lt;- rnorm(n = 10, mean = 2, sd = 20)\n2t(x) * x\n3names(x) &lt;- LETTERS[1:length(x)]\n4x[x &gt; 0]\n5x[1:3]\n\n\n\n1\n\nVector creation \\(\\boldsymbol{\\mathrm{x}}\\) by different approaches. Sequences, repeats, repetitions and sampling\n\n2\n\nTransposition of vector.\n\n3\n\nNaming elements of a vector.\n\n4\n\nSelection of elements from a vector based on a condition.\n\n5\n\nSelection of elements from a vector based on an index.\n\n\n\n\n         [,1]     [,2]    [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n[1,] 442.1663 256.2098 1154.34 82.19331 94.33618 385.5086 678.0642 106.0194\n         [,9]    [,10]\n[1,] 29.34096 1224.797\n       A        G        H        J \n21.02775 26.03966 10.29657 34.99709 \n        A         B         C \n 21.02775 -16.00655 -33.97558 \n\n\n\n\nCode\n```{r}\n#| label: test-code-annotation\nV &lt;- vector(mode = \"numeric\", length = 0) # empty numeric vector creation\nV[1] &lt;- \"A\"\n```\n\n\n\n\n\nMatrices and arrays\nIf the object has more than one dimension, it is treated as an array. A special type of array is a matrix. Both object types have accompanying functions like colSums(), rowMeans().\n\nList of matrix bounded functions\n\n\nFunction\nMeaning\n\n\n\n\nnrow(), ncol()\nnumber of rows, columns in matrix\n\n\ndim()\ndtto\n\n\ndet()\nmatrix determinant\n\n\neigen()\neigenvalues, eigenvectors\n\n\ncolnames()\ncolumn names in matrix\n\n\nrowSums()\nrow sums in matrix\n\n\ncolMeans()\ncolumn means of matrix\n\n\nM[m, ]\nSelection of \\(m\\)-th row of matrix\n\n\nM[ ,n]\nSelection of \\(n\\)-th column of matrix\n\n\n\n\n\nCode\nx &lt;- c(1:10)\n1dim(x) &lt;- c(2, 5)\nx\n\n\n\n1\n\nConversion to \\(2\\times 2\\) dimension\n\n\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n\n\n\nCode\nM &lt;- matrix(data = 0, nrow = 5, ncol = 2) # empty matrix creation\nM[1, 1] &lt;- 1                              # add single value at origin\nM[, 1] &lt;- 1.5                             # store 1.5 to the whole first column\nM[c(1,3), 1:2] &lt;- rnorm(2)                # store random numbers to first two rows\n\ncolMeans(M) \n## [1] 1.1074673 0.2074673\nrowSums(M)\n## [1] 0.6401174 1.5000000 1.4345554 1.5000000 1.5000000\n\n\nIt is possible to have matrices containing any data type, e.g.\n\\[\nM = \\left(\\begin{matrix}\n\\mathrm{A} & \\mathrm{B}\\\\\n\\mathrm{C} & \\mathrm{D}\n\\end{matrix}\\right),\\qquad\nN = \\left(\\begin{matrix}\n1+i & 5-3i\\\\\n10+2i & i\n\\end{matrix}\\right)\n\\]\n\n\nData frames\ndata.frame structure is the workhorse of elementary data processing. It is a possibly heterogenic table-like structure, allowing storage of multiple data types (even other structures) in different columns. A column in any data frame is called a variable and row represents a single observation. If the data suffice this single condition, we say they are in tidy format. Processing tidy data is a big topic withing the R community and curious reader is encouraged to follow the development in tidyverse package ecosystem.\n\n\nCode\nthaya &lt;- data.frame(date = NA, \n                    runoff = NA, \n                    precipitation = NA) # new empty data.frame with variables 'date', 'runoff', 'precipitation' and 'temperature'\n#thaya$runoff &lt;- rnorm(100, 1, 2)\n\n\n\n\nLists\nList is the most general basic data structure. It is possible to store vectors, matrices, data frames and also other lists within a list. List structure does not pose any limitations on the internal objects lengths.\n\n\nCode\nl &lt;- list() # empty list creation \nl[\"A\"] &lt;- 1\nprint(l)\n\n\n$A\n[1] 1\n\n\nCode\nl$A &lt;- 2\nprint(l)\n\n\n$A\n[1] 2\n\n\n\n\nOther objects\nAlthough R is intended as functional programming language, more than one object oriented paradigm is implemented in the language. As new R users we encounter first OOP system in functions like summary and plot, which represent so called S3 generic functions. We will further work with S4 system when processing geospatial data using proxy libraries like sf and terra. The OOP is very complex and will not be further discussed within this text. For further study we recommend OOP sections in Advanced R by Hadley Wickham.\n\n\n\n2.3.6 Control flow\nCondition and cycles govern the run of the general flow of calculation, they are the building blocks of algorithms.\n\n2.3.6.1 Conditions\nA condition in code creates branching of computation. Placing a condition creates at least two options from which only one is to be satisfied. The condition is created either by if()/ifelse() or switch() construct. We can again call for a snippet from RStudio help resulting in\n\n\nCode\nif (condition) {\n  ...\n}\n\nswitch (object,\n  case = action\n)\n\nifelse(test, TRUE, FALSE)\n\n\n\nif()\n\n\nCode\nA &lt;- 1\nif(A &gt;= 1) {\n  cat(\"A larger than or equal 1.\")\n}\n\n\nA larger than or equal 1.\n\n\n\n\nCode\nA &lt;- 5\n1if(A &gt;= 2) {\n  cat(\"A is larger than or equal 2.\")\n} else if(A &gt; 2) {\n  cat(\"A is larger than 2.\")\n}\n\n\n\n1\n\nThe chain of conditions will close at the first evaluation which happens to be TRUE.\n\n\n\n\nA is larger than or equal 2.\n\n\n\n\nifelse()\nVectorized condition, in general looks like\n\n\nCode\nx &lt;- -5:5\ncat(\"Element x + 3 is more than 0: \", ifelse(x - 3 &gt; 0, yes = \"Yes\", no = \"No\"))\n\n\nElement x + 3 is more than 0:  No No No No No No No No No Yes Yes\n\n\n\n\n\nswitch()\n\n\nCode\nvariant &lt;- \"B\"\n2 * (switch(\n      variant,\n1        \"A\" = 2,\n2        \"B\" = 3))\n\n\n\n1\n\n“A” variant did not happen,\n\n2\n\ninstead the “B” variant is truthful, so the expression is evaluated as \\(2\\cdot 3 = 6\\)\n\n\n\n\n[1] 6\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCreate a following grading scheme:\n\n\n\nGrade\nResult\n\n\n\n\nA\n90 % - 100 %\n\n\nB\n75 % - 89 %\n\n\nC\n60 % - 74 %\n\n\nD\n&lt; 60 %\n\n\n\n\n\n\n\n\n2.3.7 Loops\nLoops (cycles) provide use with the ability to execute single statement of a block of code in {} multiple times. There are three key words for loop construction. They differ in use cases.\n\nfor cycle\nProbably the most common loop is used when you know the number of iterations prior to calling. The iteration is therefore explicitly finite.\n\n\nCode\nfor (variable in vector) {\n  ...\n}\n\n\nAn example\n\n\nCode\nfor(i in 1:4) cat(i, \". iteration\", \"\\n\", sep = \"\")\n\n\n1. iteration\n2. iteration\n3. iteration\n4. iteration\n\n\n\n\nwhile cycle\nwhile is used in when it is impossible to state how many times something should be repeated. The case is rather in the form while some condition is or is not met, repeat what is inside the body. It is also used in intentionally infinite loop e.g. operating systems.\n\n\nCode\ni &lt;- 1\nwhile(i &lt; 5) {\n  cat(\"Iteration \", i, \"\\n\", sep = \"\")\n  i &lt;- i + 1\n}\n\n\nIteration 1\nIteration 2\nIteration 3\nIteration 4\n\n\n\n\nrepeat cycle\nIn the cases when we need the repetition at least once, we will evaluate the code inside until a condition is met.\n\n\nCode\ni &lt;- 1\n1repeat {\n  cat(\"Iteration\", i, \"\\n\")\n  i &lt;- i + 1\n2  if(i &gt;= 5) break\n}\n\n\n\n1\n\nExecute in loop,\n\n2\n\nif a condition is met, break stops the cycle.\n\n\n\n\nIteration 1 \nIteration 2 \nIteration 3 \nIteration 4 \n\n\n\n\nbreak and next\nThere are two statements which controls the iteration flow. Anytime break is called, the rest of the body is skipped and the loop ends. Anytime next is called, the rest of the body is skipped and next iteration is started.\n\n\n\n\n\n\nExercise\n\n\n\n\nCreate a cycle, which for the numbers \\(x={1, 2, 3, 4, 5}\\) writes out \\(x^3\\).\nCalculates the cumulative sum for these.s\nCalculates the factorial number for the number x.\nWithe the help of readline() function (requests a number from the user), prints the number. If the given number is negative, the loop ends."
  },
  {
    "objectID": "02_statistics.html#exploratory-data-analysis-eda",
    "href": "02_statistics.html#exploratory-data-analysis-eda",
    "title": "3  Processing of hydrological dataset",
    "section": "3.1 Exploratory Data Analysis (EDA)",
    "text": "3.1 Exploratory Data Analysis (EDA)\nEDA is not a strictly formalized set of procedures, rather it is an general approach on how to study, describe, extract, evaluate and report about the data. Hence the workflow is specific to data in process.\nWhen studying data, we will look at certain statistical features, which represent esteemed characteristics of the whole data set.\n\n\nCode\n1set.seed(123)\n2x &lt;- rnorm(n = 30, mean = 50, sd = 10)\nx\n##  [1] 44.39524 47.69823 65.58708 50.70508 51.29288 67.15065 54.60916 37.34939\n##  [9] 43.13147 45.54338 62.24082 53.59814 54.00771 51.10683 44.44159 67.86913\n## [17] 54.97850 30.33383 57.01356 45.27209 39.32176 47.82025 39.73996 42.71109\n## [25] 43.74961 33.13307 58.37787 51.53373 38.61863 62.53815\n\n\n\n1\n\nUsing the set seed will ensure that we will get the same generated numbers every time.\n\n2\n\nSome random-generated numbers from the Normal distribution.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nTry to play with the function rnorm() by chaging the parameters. What is the function of set.seed()?\n\n\n\n\n3.1.1 Measures of location\nThe farthest one can reduce data, while still retaining any information at all is by a single value. They describe a central tendency of the data.\n\n3.1.1.1 Mean\nThe arithmetic mean (or average) is simply the sum of observations devided by the number of observations.\n\\[\n\\bar{x} = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} x_i\n\\]\n\n\n3.1.1.2 Median\nThe median is another central tendency based on the sorted data set. It is the \\(50\\)th quantile in the data. A half of the values is smaller than median, and a half is larger.\n\n\nCode\nmedian(x)\n## [1] 49.26267\n(sort(x)[length(x)/2] + sort(x)[length(x)/2 + 1])/2\n## [1] 49.26267\n\n\n\n\n3.1.1.3 Mode\nFor continuous variables in real numbers, computation of mode can be done either by cutting the values into meaningful bins or using the kernel density estimate.\n\n\nCode\ncut(x, breaks = 10)\n\n\n [1] (41.6,45.3] (45.3,49.1] (64.1,67.9] (49.1,52.9] (49.1,52.9] (64.1,67.9]\n [7] (52.9,56.6] (34.1,37.8] (41.6,45.3] (45.3,49.1] (60.4,64.1] (52.9,56.6]\n[13] (52.9,56.6] (49.1,52.9] (41.6,45.3] (64.1,67.9] (52.9,56.6] (30.3,34.1]\n[19] (56.6,60.4] (41.6,45.3] (37.8,41.6] (45.3,49.1] (37.8,41.6] (41.6,45.3]\n[25] (41.6,45.3] (30.3,34.1] (56.6,60.4] (49.1,52.9] (37.8,41.6] (60.4,64.1]\n10 Levels: (30.3,34.1] (34.1,37.8] (37.8,41.6] (41.6,45.3] ... (64.1,67.9]\n\n\nCode\ny &lt;- cut(x, breaks = seq(from = -10, to = 100, by = 10))\ntable(y)\n\n\ny\n (-10,0]   (0,10]  (10,20]  (20,30]  (30,40]  (40,50]  (50,60]  (60,70] \n       0        0        0        0        6        9       10        5 \n (70,80]  (80,90] (90,100] \n       0        0        0 \n\n\nCode\nbarplot(height = table(y))\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nChange the cut() function up so that the vector \\(x\\) is cut into 20 bins.\nAdd a single number to vector x so that the sample mean rises to \n\n\n\n\n\n\n3.1.1.4 \\(n^{\\mathrm{th}}\\) quantile\n\n\nCode\nquantile(x, probs = c(0.1, 0.9))\n\n\n     10%      90% \n38.49171 62.84304 \n\n\n\n\n\n3.1.2 Measures of spread\nWith measures of variability we describe the spread of the data around its central tendency. Some degree of variability is a natural occurring phenomenon.\n\n3.1.2.1 Variation range\nThe simples measure of spread is the variation range. It is computed as the difference of both end extremes of the data.\n\\[\nR = \\max{x} - \\min{x}\n\\]\n\n\nCode\nmax(x) - min(x)\n\n\n[1] 37.5353\n\n\n\n\n3.1.2.2 Variance and sample variance\nWe calculate the variance as the average of the squares of the deviations of individual character values from their arithmetic mean \\(\\bar{x}\\).\n\\[\n\\begin{array}{rl}\n\\sigma^2 =& \\dfrac{1}{n}\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2\\\\\ns^2 =& \\dfrac{1}{n-1}\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2\n\\end{array}\n\\] Usually we do not have the ability to work with the whole population data. \\(s^2\\), the sample variance formula needs to be used for and unbiased estimate.\n\n\n3.1.2.3 Standard deviation\nThe standard deviation is defined as square root of variance.\n\n\nCode\nsd(x)\n## [1] 9.810307\nvar(x)^0.5\n## [1] 9.810307\n\n\n\n\n3.1.2.4 Coefficient of variation\nThe coefficient of variation is given by the ratio of the standard deviation and the arithmetic mean, and it follows from the definition of this ratio that it is a dimensionless indicator.\n\\[\n\\nu = \\frac{s}{\\bar{x}}\n\\]\n\n\n3.1.2.5 Interquartile range\nThe \\(\\text{IQR}\\) is the upper quartile (\\(75\\)th percentile) minus the lower quartile (\\(25\\)th percentile)\n\\[\n\\text{IQR} = Q_{\\text{III}} - Q_{\\text{I}}\n\\]\n\n\nCode\nIQR(x)\n\n\n[1] 11.60016\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nUsing the knowledge from the Chapter 2 create own function to calculate the sample variance according the formula."
  },
  {
    "objectID": "02_statistics.html#hydrological-data",
    "href": "02_statistics.html#hydrological-data",
    "title": "3  Processing of hydrological dataset",
    "section": "3.2 Hydrological data",
    "text": "3.2 Hydrological data\nDatasets containing hydrological data are quite commonly, although not exclusively, in tabular (rectangular) shape. Let’s take a look at sample data from MOPEX. It is a fairly extensive curated dataset.\nThis dataset covered some 438 catchments in daily step with measured discharge. The dataset used to be publicly available at https://hydrology.nws.noaa.gov/pub/gcip/mopex/US_Data/Us_438_Daily, the site is unavailable now. Several even more extensive datasets recently came out.\n\n\nCode\n1url &lt;- \"data/01138000.dly\"\n2mpx_01138000 &lt;- read.fwf(file = url,\n                widths = c(8, rep(10, times = 5)),\n                header = FALSE)\n3names(mpx_01138000) &lt;- c(\"date\", \"prec\", \"pet\", \"r\", \"tmax\", \"tmin\")\n4mpx_01138000$date &lt;- gsub(x = mpx_01138000$date,\n                          pattern = \" \",\n                          replacement = \"0\")\nmpx_01138000$date &lt;- as.Date(x = mpx_01138000$date,\n                             format = \"%Y%m%d\")\n5mpx_01138000[which(mpx_01138000$r &lt; 0), \"r\"] &lt;- NA\n6rbind(head(mpx_01138000, n = 5),\n      tail(mpx_01138000, n = 5))\n\n\n\n1\n\nThis file is stored locally as part of this site. You have to change to your path.\n\n2\n\nLoad the data in fixed width format using the read.fwf() function.\n\n3\n\nProvide variable names, since there aren’t any.\n\n4\n\nNow there is a trouble with dates. If you evaluate sequentially, you could see that the date format expects “YYYY-mm-dd” format, we have to fill the blank spots with zeroes to comply.\n\n5\n\nAfter that, some variables contain non-physical values like -9999, these would make troubles in statistical processing, we have to replace them with NA (not assigned).\n\n6\n\nLet’s display the 5 first and 5 last rows of the result.\n\n\n\n\n            date prec   pet      r    tmax     tmin\n1     1948-01-01 0.00 0.080 0.2620 -2.9167 -11.2556\n2     1948-01-02 4.44 0.081 0.2501 -2.9556 -11.8500\n3     1948-01-03 4.74 0.081 0.2501 -0.1778  -5.6444\n4     1948-01-04 0.00 0.081 0.2620 -1.8778  -4.2222\n5     1948-01-05 0.00 0.083 0.2501  0.8778  -4.3389\n20450 2003-12-27 0.59 0.095     NA  0.1556  -5.6667\n20451 2003-12-28 0.00 0.091     NA  1.6444 -12.8111\n20452 2003-12-29 0.20 0.088     NA  4.1667 -12.5500\n20453 2003-12-30 2.03 0.086     NA  4.8889  -9.4222\n20454 2003-12-31 0.84 0.084     NA  3.5278  -1.7500\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nData coming from field measurements are usually accompanied with metadata. This could be information about when the data were collected, the site location, precision, missing values etc. Sometimes the metadata are in a header of the file. Download and process this file in a similar manner using help of this read.txt() function."
  },
  {
    "objectID": "02_statistics.html#aggregation-and-summation",
    "href": "02_statistics.html#aggregation-and-summation",
    "title": "3  Processing of hydrological dataset",
    "section": "3.3 Aggregation and summation",
    "text": "3.3 Aggregation and summation\nLet’s add some features to the data. For the purpose of the analysis it would be beneficial to add year, month, quarters, decade and hyear as hydrological year.\n\n\nCode\n1mpx_01138000$year &lt;- as.integer(format(mpx_01138000$date, \"%Y\"))\nmpx_01138000$month &lt;- as.integer(format(mpx_01138000$date, \"%m\"))\n\n\n\n1\n\nWith the help of special characters we are extracting the years and months consecutively.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nAdd quart variable to mpx_01138000 using quarters() function.\nProvide the fraction of days with no precipitation: \nHow many freezing days (Tmin &lt; 0) occured? \nHow many freezing days occured in late spring (May) and summer? \nFind 3 highest  and 3 lowest temperatures  (delimit by a comma).\n\n\n\n\n\n\nCode\n1mpx_01138000$hyear &lt;- ifelse(test = mpx_01138000$month &gt; 10,\n                             yes = as.integer(mpx_01138000$year + 1),\n                             no = as.integer(mpx_01138000$year))\n\n\n\n1\n\nThe hydrological year in Czechia is defined from \\(1^{\\mathrm{st}}\\) November to \\(31^{\\mathrm{st}}\\) October next year.\n\n\n\n\nThese functions are based on grouping. In hydrology, the natural groups involve - stations/basins, decades/years/months, groundwater regions, etc.\n\n3.3.1 Box-plot\nCome handy, when we want to visualize the important quantiles related to any categorical variable.\n\n\nCode\nstation &lt;- read.delim(file = \"./data/6242910_Q_Day.Cmd.txt\", \n                      skip = 36, \n                      header = TRUE, \n                      sep = \";\", \n                      col.names = c(\"date\", \"time\", \"value\"))\nstation$date &lt;- as.Date(station$date, \n                        format = \"%Y-%m-%d\")\n\nstation_agg &lt;- aggregate(station$value ~ as.factor(data.table::month(station$date)), \n                         FUN = \"quantile\", \n                         probs = c(0.1, 0.25, 0.5, 0.75, 0.9))\n\nnames(station_agg) &lt;- c(\"Month\", \"Discharge\")\npar(font.main = 1, \n    adj = 0.1, \n    xaxs = \"i\", \n    yaxs = \"i\")\nboxplot(data = station_agg, \n        Discharge ~ Month, \n        main = \"Distribution of discharge in months\", \n        frame = FALSE, \n        ylim = c(0,20), \n        xlab = \"\", \n        ylab = bquote(Discharge), font.main = 1)\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Q-Q plot\nThe so called rankit graph produces a quantile-quantile graph of the values from selected data. This one can compare if the distribution of the data fit with the assumed distribution, e.q. Normal. qqline then adds the theoretical line.\n\n\nCode\npar(font.main = 1, \n    adj = 0.1, \n    xaxs = \"i\", \n    yaxs = \"i\")\nqqnorm(mpx_01138000$tmax, \n       pch = 21, \n       col = \"black\", \n       bg = \"lightgray\", cex = 0.5, \n       main = \"\")\nqqline(mpx_01138000$tmax)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Empirical distribution function\nLarge portion of the data which is processed in Hydrology originates from time series of streamflow or runoff. This enables us to construct empirical probability of exceeding certain value in the data \\(P(X\\geq x_k)\\), simply using the well know definition\n\\[\nP = \\dfrac{m}{n}\n\\] where \\(m\\) is the number of reaching or exceeding of value \\(x_k\\) and \\(n\\) is the length of the series. This equation is valid strictly for \\(n \\rightarrow \\infty\\).\nThere are several empirical formulas in use to calculate the empirical exceedance probability like the one from Čegodajev\n\\[\nP = \\dfrac{m - 0.3}{n + 0.4}\n\\] In R we can utilize a highe order function called ecdf()\n\n\nCode\n1ecdf_data &lt;- ecdf(na.omit(mpx_01138000$r))\n2ecdf_data(35)\n\n\n\n1\n\nCreate the empirical cumulative distribution function (ECDF) for the data.\n\n2\n\nCalculate percent of data lower than 35.\n\n\n\n\n[1] 0.9998328\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.4 Exceedance curve\nProvides an information on how many times or for how long a streamflow value was reached or exceeded within a certain period.\n\n\nCode\nr_sorted &lt;- na.omit(sort(mpx_01138000$r, decreasing = TRUE))\n\n# Empirical\nn &lt;- length(r_sorted)\nexceedance_prob &lt;- 1:n / n\n\nrank &lt;- seq(1:n)\nreturn_period &lt;- (n + 1) / rank\nexceedance_prob &lt;- 1 / return_period\n\nplot(x = exceedance_prob, \n     y = r_sorted, \n     type = \"l\", \n     xlab = \"Exceedance Probability\", \n     ylab = \"Runoff Value\", \n     main = \"Empirical Exceedance Curve\")"
  },
  {
    "objectID": "02_statistics.html#hydroclimatic-indices",
    "href": "02_statistics.html#hydroclimatic-indices",
    "title": "3  Processing of hydrological dataset",
    "section": "3.4 Hydroclimatic indices",
    "text": "3.4 Hydroclimatic indices\nOriginated from the combination of aggregation and summation methods and the measures of location and dispersion.\n\n3.4.1 Runoff coefficient\nThe concept of runoff coefficient comes from the presumption, that over long-time period\n\\[\n\\varphi [-] = \\dfrac{R}{P}\n\\] where \\(R\\) represents runoff and \\(P\\) precipitation in long term, typically 30 year so called Climatic period, which is a decadal 30 year reference moving window; the latest being 1991–2020.\n\n\nCode\nR &lt;- mean(aggregate(r ~ year, FUN = mean, data = mpx_01138000)[, \"r\"])\nP &lt;- mean(aggregate(prec ~ year, FUN = mean, data = mpx_01138000)[, \"prec\"])\n\nR/P\n\n\n[1] 0.493947\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCalculate the runoff coefficient \\(\\varphi\\) for the two latest decades separately.\n\n\n\n\n\n3.4.2 Flow duration curve\nThis is an essential curve in energy as well as river ecology sector.\n\n\nCode\nM &lt;- c(30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 355, 364)\nm_day &lt;- function(streamflow) {\n  quantile(na.omit(streamflow), 1 - M/365.25)\n}\nplot(M, m_day(mpx_01138000$r), \n     type = \"b\", \n     pch = 21,\n     bg = \"darkgray\",\n     xlab = \"\", \n     ylab = \"Discharge\", \n     main = \"M-day discharge\")"
  },
  {
    "objectID": "03_gis.html#whiteboxtools",
    "href": "03_gis.html#whiteboxtools",
    "title": "4  GIS in Hydrology",
    "section": "4.1 WhiteboxTools",
    "text": "4.1 WhiteboxTools\nIn this session we use the WhiteboxTools (WBT) a modern and advanced geospatial package, tools collection, which contains ~450 functions. The WBT has an interface to both R and Python.\nThe tool can be downloaded from http://whiteboxgeo.com/. We have to install through the packge. First the necessary packages need to be installed, the installation require compilation and take some time. ## Raster and vector data\nRaster data are represented by a matrix of pixels (cells) with values. Raster is used for data which display continuous information across an area which cannot be easily divided into vector features. For the purpose of watershed delineation the raster input of Digital Elevation Model is used."
  },
  {
    "objectID": "03_gis.html#watershed-delineation",
    "href": "03_gis.html#watershed-delineation",
    "title": "4  GIS in Hydrology",
    "section": "4.2 Watershed delineation",
    "text": "4.2 Watershed delineation\nThe process of delineation is the first step in basin description. One simply has to delineate the domain\nThe step-by-step process involves:\n\n\nAcquiring digital elevation model of area\n\nPit and sink removal\n\nFlow accumulation calculation\n\nFlow direction calculation\n\nOutlet identification\n\nDelineation towards specified outlet\n\n\nLet’s start with whitebox package that contains an API to the WhiteboxTools executable binary.\nWe need to reference path to the executable\nExcept for the whitebox package, some other packages for general work with spatial data are necessary. The packages terra and sf are needed for working with the raster and vector data. They also provide access to PROJ, GEOS and GDAL which are open source libraries that handle projections, format standards and provide geoscientific calculations. And the package tmap makes plotting both raster and vector layers very easy."
  },
  {
    "objectID": "03_gis.html#watershed-delineation-workflow",
    "href": "03_gis.html#watershed-delineation-workflow",
    "title": "4  GIS in Hydrology",
    "section": "4.3 Watershed delineation workflow",
    "text": "4.3 Watershed delineation workflow\nWe will install the whitebox tools through the whitebox package. Other than this package we will need the sf and terra in order to deal with the vector and raster data formats and ggplot2 for general visualisation and map-making.\nWe have to install the sought packages prior the first usage. None of them is actually contained in the default R installation.\n\n\nCode\nlapply(\n  X = setdiff(\n    c(\"whitebox\", \"terra\", \"sf\", \"ggplot2\", \"tidyterra\", \"scico\"),\n    installed.packages()[, \"Package\"]),\n  FUN = install.packages)\n\n\nlist()\n\n\nAfter the successful installation, the library function load the namespace of the packages and allows their exported functions to be used directly.\n\n\nCode\n1library(whitebox)\n2library(terra)\n## terra 1.7.78\n3library(sf)\n## Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n4library(ggplot2)\n5library(tidyterra)\n## \n## Attaching package: 'tidyterra'\n## The following object is masked from 'package:stats':\n## \n##     filter\n6library(scico)\n\n\n\n1\n\nLoad the Whitebox API package.\n\n2\n\nLoad the terra package for raster use.\n\n3\n\nLoad the sf package for vector use.\n\n4\n\nLoad the ggplot2 plotting functions for layers.\n\n5\n\nLoad the tidyterra for the terra objects and ggplot2 compatibility.\n\n6\n\nLoad scientific colors library.\n\n\n\n\nThe paths on your OS may vary, you may need to change the direction of the installation.\n\n\nCode\nwd_path &lt;- \"~/Desktop/GIS\"\n\n5wbt_init(exe_path = paste(wd_path, \"WBT/whitebox_tools\", sep = \"/\"))\n\n6check_whitebox_binary()\n## [1] TRUE\n\n\n\n5\n\nWhitebox needs the information where the data executable is stored.\n\n6\n\nBinary check. Returns TRUE if found.\n\n\n\n\n\n4.3.0.1 Sample data\n\n\nCode\n1dem &lt;- rast(whitebox::sample_dem_data())\nwriteRaster(dem, paste(wd_path, \"dem.tif\", sep = \"/\"), overwrite = TRUE)\n2ggplot() +\n  geom_spatraster(data = dem) +\n  scale_fill_scico(palette = \"turku\")\n\n\n\n1\n\nUse the rast() function to load the data\n\n2\n\nPlot via the tmap workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.0.2 DEM workflow\nThe digital elevation model has to be adjusted for the watershed delineation algorithm to be able to run successfully.\nIt’s good to specify a path to working directory. It has to be somewhere where you as a user have access to write files. Be sure the folder exists.\n\n\nCode\n\n1wbt_fill_depressions_wang_and_liu(dem = paste(wd_path,\n                                              \"dem.tif\",\n                                              sep = \"/\"),\n                                  output = paste(wd_path,\n                                                 \"filled_depresions.tif\",\n                                                 sep = \"/\"))\n\n2wbt_d8_pointer(dem = paste(wd_path,\n                           \"filled_depresions.tif\",\n                           sep = \"/\"),\n               output = paste(wd_path,\n                              \"d8pointer.tif\",\n                              sep = \"/\"))\n\nwbt_d8_flow_accumulation(input = paste(wd_path,\n                                       \"filled_depresions.tif\",\n                                       sep = \"/\"),\n                         output = paste(wd_path,\n                                        \"flow_accu.tif\",\n                                        sep = \"/\"),\n                         out_type = \"cells\")\n\nwbt_extract_streams(flow_accum = paste(wd_path,\n                                       \"flow_accu.tif\",\n                                       sep = \"/\"),\n                    output = paste(wd_path,\n                                   \"streams.tif\",\n                                   sep = \"/\"),\n                    threshold = 200)\n\n\n\n1\n\nThis algorithm involves removing flat areas and filling depressions, thus producing hydrologically corrected DEM\n\n2\n\nPointer is"
  },
  {
    "objectID": "03_gis.html#gauge",
    "href": "03_gis.html#gauge",
    "title": "4  GIS in Hydrology",
    "section": "4.4 Gauge",
    "text": "4.4 Gauge\nWe have the raster prepared for the delineation, now we need to provide a point layer with the gauge, to which the watershed should be delineated. The point has to be placed at the stream network. We will create the layer from scratch.\n\n\nCode\ngauge &lt;- st_sfc(st_point(x = c(671035, 4885783), \n                               dim = \"XY\"), \n                      crs = st_crs(26918))\nst_write(gauge, dsn = paste(wd_path, \"gauge\", \n                            sep = \"/\"), \n         driver = \"ESRI Shapefile\", \n         append = FALSE)\n\n\nDeleting layer `gauge' using driver `ESRI Shapefile'\nWriting layer `gauge' to data source \n  `/Users/petrmbpro/Desktop/GIS/gauge' using driver `ESRI Shapefile'\nWriting 1 features with 0 fields and geometry type Point.\n\n\n\n\nCode\nggplot() +\n  geom_spatraster(data = dem) +\n  scico::scale_fill_scico(palette = \"turku\") +\n  geom_sf(data = gauge, color = \"orangered\")\n\n\n\n\n\n\n\n\n\nIf the point is not located directly in the stream, it could cause troubles. The Jenson snap pour makes sure to move the point to the nearest stream pixel.\n\n\nCode\nwbt_jenson_snap_pour_points(pour_pts = paste(wd_path, \n                                             \"gauge/gauge.shp\", \n                                             sep = \"/\"), \n                            streams = paste(wd_path, \n                                            \"streams.tif\", \n                                            sep = \"/\"), \n                            output = paste(wd_path, \n                                           \"gauge_snapped.shp\", \n                                           sep = \"/\"), \n                            snap_dist = 1000)\n\n\nNow everything is set to delineate the watershed with using the gauge and D8 pointer.\n\n\nCode\nwbt_watershed(d8_pntr = paste(wd_path, \"d8pointer.tif\", sep = \"/\"),\n              pour_pts = paste(wd_path, \"gauge_snapped.shp\", sep = \"/\"), \n              output = paste(wd_path, \"watershed.tif\", sep = \"/\"))\n\n\nThe watershed is usually used in vector format, but now it is in raster. Let’s finish with the conversion.\n\n\nCode\nwtshd &lt;- rast(paste(wd_path, \"watershed.tif\", sep = \"/\"))\nwatershed &lt;- terra::as.polygons(wtshd)"
  },
  {
    "objectID": "03_gis.html#river-morphology",
    "href": "03_gis.html#river-morphology",
    "title": "4  GIS in Hydrology",
    "section": "4.5 River morphology",
    "text": "4.5 River morphology\nUnder the term river morphology we understand the description of the shape of river channels. Hydrologists use indices such as stream length or Strahler order.\n\n\nCode\nwbt_strahler_stream_order(d8_pntr = paste(wd_path, \"d8pointer.tif\", sep = \"/\"), \n                          streams = paste(wd_path, \"streams.tif\", sep = \"/\"), \n                          output = paste(wd_path, \"strahler.tif\", sep = \"/\"))"
  },
  {
    "objectID": "03_gis.html#results",
    "href": "03_gis.html#results",
    "title": "4  GIS in Hydrology",
    "section": "4.6 Results",
    "text": "4.6 Results\n\n\nCode\nresults &lt;- list.files(wd_path, pattern = \"tif$\", full.names = TRUE)\nr &lt;- lapply(results, rast)\nr1 &lt;- rast(results[2])\nr2 &lt;- rast(results[1])\nr3 &lt;- rast(results[4])\nr4 &lt;- rast(results[6])\nr5 &lt;- watershed\nr6 &lt;- rast(results[5])\n\n\np1 &lt;- ggplot() +\n  ggtitle(label = \"a) Corrected DEM\") +\n  geom_spatraster(data = r1, show.legend = FALSE) +\n  scale_fill_scico(palette = \"lapaz\") +\n  theme_minimal()\np2 &lt;- ggplot() +\n  ggtitle(label = \"b) D8 Pointer\") +\n  geom_spatraster(data = r2, show.legend = FALSE) +\n  scale_fill_scico(palette = \"hawai\") +\n  theme_minimal() \np3 &lt;- ggplot() +\n  ggtitle(label = \"c) Flow Accumulation\") +\n  geom_spatraster(data = r3, show.legend = FALSE) +\n  scale_fill_scico(palette = \"oslo\", direction = -1) +\n  theme_minimal() \np4 &lt;- ggplot() +\n  ggtitle(label = \"d) Identified Streams\") +\n  geom_spatraster(data = r4, show.legend = FALSE) +\n  scale_fill_viridis_c(na.value = \"white\") +\n  theme_minimal() \np5 &lt;- ggplot() +\n  ggtitle(label = \"e) Watershed\") +\n  geom_spatraster(data = r1, show.legend = FALSE) +\n  scale_fill_scico(palette = \"lapaz\") +\n  geom_spatvector(data = r5, show.legend = FALSE) +\n  theme_minimal() \np6 &lt;- ggplot() +\n  ggtitle(label = \"f) Strahler Order\") +\n  geom_spatraster(data = r6, show.legend = FALSE) +\n  scale_fill_viridis_b(na.value = \"white\") +\n  theme_minimal() \n\ncowplot::plot_grid(p1, NULL, p2, \n                   p3, NULL, p4, \n                   p5, NULL, p6, \n                   nrow = 3, align = \"hv\", rel_widths = c(1,0,1), greedy = TRUE)\n\n\n\n\n\ndata/output.png"
  },
  {
    "objectID": "04_interpolation.html#inverse-distance-weighting-idw",
    "href": "04_interpolation.html#inverse-distance-weighting-idw",
    "title": "5  Data interpolation",
    "section": "5.1 Inverse distance weighting (IDW)",
    "text": "5.1 Inverse distance weighting (IDW)\nIDW is a simple deterministic interpolation method, which is also non-parametric. The interpolated points are calculated with a weighted average of the values which are at disposal. The space is a weighted average of the distribution of points and the weight assigned to each point decrease with increasing distance from the interpolated point.\nThe point value \\(Z_p\\) is calculated with the knowledge of \\(z_i\\) and distance \\(d_i\\) to the power of \\(P\\).\n\\[\nZ_L = \\dfrac{\\sum\\limits_{i=1}^{n}\\dfrac{z_i}{d_i^p}}{\\sum\\limits_{i=1}^{n}\\dfrac{1}{d_i^p}}\n\\] Let’s imagine we have three values \\(z_1 = 10\\), \\(z_2 = 15\\), \\(z_3 = 20\\) and we need to estimate a value in a location \\(L\\). The distances to the location from these points are: \\(d_1 = 3.0\\), \\(d_2 = 2.0\\), \\(d_3 = 1.5\\). The commonly used power \\(p\\) is \\(2\\).\nSo the respective values of weights for these are:\n\\[\n\\begin{array}{c}\nw_1 = \\dfrac{10}{3.0^2}\\\\\nw_2 = \\dfrac{15}{2.0^2}\\\\\nw_3 = \\dfrac{20}{1.5^2}\n\\end{array}\n\\]\nHence the numerator and denominator look as follows:\n\\[\n\\begin{array}{l}\nN = \\dfrac{10}{2.0^2} + \\dfrac{15}{2.0^2} + \\dfrac{20}{1.5^2}\\approx 2.5 + 2.2222 + 6.6666 = 11.3888\\\\\nD = 0.25 + 0.1111 + 0.4444 = 0.8055\\\\\n\\end{array}\n\\]\nSince this method is very simple, let’s calculate it on our own, in a step-by-step manner.\n\nFirst we generate some point in the spatial domain, which will represent our measurements.\nNext we create a new domain, spatially regular to which we will interpolate.\nFinally we will perform the calculations and visualize the results.\n\n\n5.1.1 Generation of random measurements network\nWe will create 25 points in the space and also assign some coordinate reference system. These points will be used for both of the methods.\n\n\nCode\nlibrary(sf)          # Spatial Feature library for spatial data\nlibrary(scico)       # Scientific palette collection called \"scico\"\n\n1n &lt;- 30\ndom &lt;- data.frame(x = runif(n, 0, 50),\n                  y = runif(n, 0, 50),\n                  value = rnorm(n, mean = 5)) |&gt;\n2  sf::st_as_sf(coords = c(\"x\", \"y\"),\n               crs = 4326)\n3plot(dom,\n     nbreaks = n + 1,\n     pal = scico(n = n,\n                 palette = \"davos\",\n                 end = 0.9,\n                 direction = -1),\n     main = \"Precipitation [mm]\",\n     pch  = 20,          # Point character selection\n     graticule = TRUE,   # Display graticules\n     axes = TRUE,        # Display plot axes\n     bgc = \"#f0f0f033\",  # Background color\n     key.pos = 1)        # Legend position\n\n\n\n1\n\nCreate \\(25\\) points with random values, which will serve as the computation origin,\n\n2\n\nstore them as SimpleFeatures object with coordinate reference system via EPSG, search in https://epsg.io\n\n3\n\nSpecify sf:::plot.sf() function and use scientific colormaps from scico package.\n\n\n\n\n\n\n\n\n\n\n\nWe do have the original points, which are scarcely distributed across the domain. Now we need a grid of new points, which will represent the centroids of a raster, on which we want to recalculate.\n\n\nCode\n1grid &lt;- st_make_grid(x = dom,\n                     cellsize = 2,\n                     crs = 4326) |&gt;\n  st_sf()\n2plot(x = dom,\n     breaks = seq(min(dom$value), max(dom$value), length.out = n),\n     pal = scico(n = n - 1,\n                 palette   = \"davos\",\n                 end       = 0.9,\n                 direction = -1),\n     main      = \"Precipitation [mm]\",\n     pch       = 20,          # Point character selection\n     graticule = TRUE,        # Display graticules\n     axes      = TRUE,        # Display plot axes\n     bgc       = \"#f0f0f033\", # Background color\n     key.pos   = 1, reset = FALSE)           # Legend position\nplot(x = st_geometry(grid), lwd = 0.1, add = TRUE, reset = TRUE)\n\n\n\n1\n\nConstruct a regular grid using extreme points as boundary limits.\n\n2\n\nPlot with the points.\n\n\n\n\n\n\n\nPoint location of Precipitation measuring stations\n\n\n\n\nNow we compute the distances between the points using the outer() function\n\n\nCode\ndistances &lt;- sapply(1:nrow(st_coordinates(grid)), function(i) {\n  sqrt((st_coordinates(grid)[i, \"X\"] - st_coordinates(dom)[, \"X\"])^2 +\n       (st_coordinates(grid)[i, \"Y\"] - st_coordinates(dom)[, \"Y\"])^2)\n})\n\n\nand use these distances for missing values computation\n\n\nCode\n1epsilon &lt;- 1e-6\ndistances &lt;- distances + epsilon\n2power &lt;- 2\n# Compute weighted sums for all grid points\nweighted_sum &lt;- sapply(1:ncol(distances), function(i) {\n  sum(dom$value / (distances[, i]^power))\n})\n\n# Compute weights sum for all grid points\nweights_sum &lt;- sapply(1:ncol(distances), function(i) { \n  sum(1 / (distances[, i]^power))\n})\n\n# Calculate interpolated values for all grid points\ninterpolated_values &lt;- weighted_sum / weights_sum\n\n# Create a spatial data frame for results\nresults &lt;- data.frame(\n  x = st_coordinates(grid)[, \"X\"],\n  y = st_coordinates(grid)[, \"Y\"],\n  value = interpolated_values\n) |&gt; \n  sf::st_as_sf(coords = c(\"x\", \"y\"), crs = 4326)\n\n# Replace NaN values with NA for plotting\nresults$value[is.nan(results$value)] &lt;- NA\n\n# Join results with the grid for visualization\njoin &lt;- st_join(grid, results)\n\n\n\n1\n\nSet a tolerance for when the distance is equal to 0.\n\n2\n\nA power for the distance.\n\n\n\n\nAt last we visualize the results.\n\n\nCode\nplot(x = join, \n     breaks = seq(min(results$value), max(results$value), length.out = n),\n     lwd = 0.001,\n     pal = scico(n - 1, \n                 palette = \"davos\",\n                 end       = 0.9, \n                 direction = -1),\n     main      = \"Precipitation [mm]\", \n     #pch       = 20,          # Point character selection\n     graticule = TRUE,        # Display graticules\n     axes      = TRUE,        # Display plot axes\n     bgc       = \"#f0f0f033\", # Background color\n     key.pos   = 1, \n     reset = FALSE)           # Legend position"
  },
  {
    "objectID": "04_interpolation.html#gstat",
    "href": "04_interpolation.html#gstat",
    "title": "5  Data interpolation",
    "section": "5.2 gstat",
    "text": "5.2 gstat\nThere are many libraries listed in CRAN geostatistics task view. One of these is called gstat, it was developed and is maintained by Edzer Pebesma, who is also behind the raster and terra packages. The gstat package contains functions no olny for interpolations.\n\n\nCode\nlibrary(gstat)\nidw_res &lt;- gstat::idw(formula = value ~ 1, \n                      locations = dom, \n                      newdata = grid)\n\n\n[inverse distance weighted interpolation]\n\n\nCode\nidw_res &lt;- st_as_sf(idw_res)\nplot(idw_res |&gt; \n       dplyr::select(var1.pred), \n     breaks = seq(min(idw_res$var1.pred), \n                  max(idw_res$var1.pred), \n                  length.out = n),\n     lwd = 0.001,\n     pal = scico(n - 1, \n                 palette = \"davos\",\n                 end       = 0.9, \n                 direction = -1),\n     main      = \"Precipitation [mm]\", \n     #pch       = 20,          # Point character selection\n     graticule = TRUE,        # Display graticules\n     axes      = TRUE,        # Display plot axes\n     bgc       = \"#f0f0f033\", # Background color\n     key.pos   = 1, \n     reset = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nCode\npower &lt;- 2  # You can adjust the power parameter\nweighted_sum &lt;- apply(distances, 1, function(d) sum(dom$value / (d^power)))\nweighted_sum"
  },
  {
    "objectID": "04_interpolation.html#krigging",
    "href": "04_interpolation.html#krigging",
    "title": "5  Data interpolation",
    "section": "5.3 Krigging",
    "text": "5.3 Krigging\nAnother interpolation technique is called Krigging. Opposing to the Inverse Distance Weighted method.\n\n\nCode\nprojected_crs &lt;- 32633  # Replace with the appropriate UTM zone for your data\n\ndom_projected &lt;- st_transform(dom, crs = projected_crs)\ngrid_projected &lt;- st_transform(grid, crs = projected_crs)\n\nset.seed(123)  # For reproducibility\nidw_points &lt;- st_centroid(idw_res)\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nCode\nsampled_points &lt;- idw_points[sample(1:nrow(idw_points), 50), ]\n\ncombined_points &lt;- rbind(\n  dom,\n  sampled_points |&gt; \n  dplyr::select(value = var1.pred, geometry)\n)\ncombined_points &lt;- st_transform(combined_points, crs = projected_crs)\n\n\nAn essential part of kriging is definition of a variogram, which is a function describing the degree of spatial dependence of a spatial process. Due to the range parameter, the spatial data needs to be projected.\n\n\nCode\nvariogram &lt;- vgm(psill = 5.05, model = \"Exp\", range = 300000, nugget = 0.15)\n\n\n\n\nCode\nkrig_res &lt;- gstat::krige(\n  formula = value ~ 1,\n  locations = combined_points,\n  newdata = grid_projected,\n  model = variogram  # Use the appropriate fitted variogram model\n)\n\n\n[using ordinary kriging]\n\n\nCode\n# Step 3: Reproject the kriged result back to the original CRS (WGS84)\nkrig_res &lt;- st_transform(st_as_sf(krig_res), crs = 4326)\n# Generate unique breaks\nmin_val &lt;- min(krig_res$var1.pred, na.rm = TRUE)\nmax_val &lt;- max(krig_res$var1.pred, na.rm = TRUE)\n\n# Ensure that min and max are sufficiently different to create unique breaks\nif (max_val - min_val &gt; 0) {\n  breaks &lt;- seq(min_val, max_val, length.out = 25)\n} else {\n  breaks &lt;- unique(c(min_val, max_val))  # Fallback for nearly constant data\n}\n\n# Use the scico color palette with the correct number of colors\ncolors &lt;- scico(length(breaks) - 1, \n                palette = \"davos\", \n                end = 0.9, \n                direction = -1)\n\n# Plotting using the 'pal' argument for sf objects\nplot(\n  krig_res |&gt; dplyr::select(var1.pred), \n  breaks = breaks,\n  pal = colors,        # Use 'pal' to specify the color palette\n  lwd = 0.001,\n  main = \"Precipitation [mm]\", \n  graticule = TRUE, \n  axes = TRUE, \n  bgc = \"#f0f0f033\", \n  key.pos = 1, \n  reset = FALSE\n)"
  },
  {
    "objectID": "05_CN.html#footnotes",
    "href": "05_CN.html#footnotes",
    "title": "6  SCS Curve number method",
    "section": "",
    "text": "https://edepot.wur.nl/183157↩︎"
  }
]